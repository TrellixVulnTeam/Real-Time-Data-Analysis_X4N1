{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc6c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b91adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8fcc88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "|arrival|day|departure|    id|station_code|       station_name|          train_name|train_number|\n",
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "|   None|  1| 07:55:00|302214|          FM|KACHEGUDA FALAKNUMA|Falaknuma Lingamp...|       47154|\n",
      "|   None|  1| 18:55:00|281458|         TCR|            THRISUR|Thrissur Guruvayu...|       56044|\n",
      "|   None|  1| 15:05:00|309335|         PBR|          PORBANDAR|Porbandar Muzaffa...|       19269|\n",
      "|   None|  1| 13:30:00|283774|           R|          RAIPUR JN|  RAIPUR ITWARI PASS|       58205|\n",
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.option(\"multiline\",\"true\").json(\"./data/train_schedules.json\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53c7ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|train_number|station_code|departure|\n",
      "+------------+------------+---------+\n",
      "|       47154|          FM| 07:55:00|\n",
      "|       56044|         TCR| 18:55:00|\n",
      "|       19269|         PBR| 15:05:00|\n",
      "|       58205|           R| 13:30:00|\n",
      "+------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"train_number\", \"station_code\", \"departure\",).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a3170545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|train_number|station_code|departure|\n",
      "+------------+------------+---------+\n",
      "|       47154|          FM| 07:55:00|\n",
      "|       56044|         TCR| 18:55:00|\n",
      "|       19269|         PBR| 15:05:00|\n",
      "|       58205|           R| 13:30:00|\n",
      "+------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.train_number, df.station_code, df.departure).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "062a9ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|train_number|station_code|departure|\n",
      "+------------+------------+---------+\n",
      "|       47154|          FM| 07:55:00|\n",
      "|       56044|         TCR| 18:55:00|\n",
      "|       19269|         PBR| 15:05:00|\n",
      "|       58205|           R| 13:30:00|\n",
      "+------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"train_number\"), col(\"station_code\"), col(\"departure\")) \\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "180289c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- departure: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- train_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c913829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arrival',\n",
       " 'day',\n",
       " 'departure',\n",
       " 'id',\n",
       " 'station_code',\n",
       " 'station_name',\n",
       " 'train_name',\n",
       " 'train_number']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d63a4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      station_name|count|\n",
      "+------------------+-----+\n",
      "|                  |    2|\n",
      "|A-CABIN BONDAMUNDA|   48|\n",
      "|             ABADA|  182|\n",
      "|          ABHAIPUR|   56|\n",
      "|  ABHAYAPURI ASSAM|   32|\n",
      "|          ABJUGANJ|   14|\n",
      "+------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"station_name\").count().orderBy(\"station_name\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42fb3ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrival', 'string'),\n",
       " ('day', 'bigint'),\n",
       " ('departure', 'timestamp'),\n",
       " ('id', 'bigint'),\n",
       " ('station_code', 'string'),\n",
       " ('station_name', 'string'),\n",
       " ('train_name', 'string'),\n",
       " ('train_number', 'string')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37e6d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.withColumn(\"departure\",to_timestamp(\"departure\"))\n",
    "df= df.withColumn(\"arrival\",to_timestamp(\"arrival\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed6a94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|     arrival|timestamp|   null|\n",
      "|         day|   bigint|   null|\n",
      "|   departure|timestamp|   null|\n",
      "|          id|   bigint|   null|\n",
      "|station_code|   string|   null|\n",
      "|station_name|   string|   null|\n",
      "|  train_name|   string|   null|\n",
      "|train_number|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary table called schedules\n",
    "df.createOrReplaceTempView(\"schedules\")\n",
    "spark.sql(\"DESCRIBE schedules\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f80f2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------------+-------------------+----------+-------------------+\n",
      "|train_number|station_code|     station_name|          departure|row_number|   upcoming_arrival|\n",
      "+------------+------------+-----------------+-------------------+----------+-------------------+\n",
      "|       12301|         HWH|        HOWRAH JN|2021-04-26 16:55:00|         1|2021-04-26 16:58:00|\n",
      "|       12301|         LLH|           LILUAH|2021-04-26 16:58:00|         2|2021-04-26 17:00:00|\n",
      "|       12301|         BEQ|            BELUR|2021-04-26 17:00:00|         3|2021-04-26 17:01:00|\n",
      "|       12301|         BLY|            BALLY|2021-04-26 17:01:00|         4|2021-04-26 17:03:00|\n",
      "|       12301|         BZL|        BELANAGAR|2021-04-26 17:03:00|         5|2021-04-26 17:05:00|\n",
      "|       12301|        DKAE|          DANKUNI|2021-04-26 17:05:00|         6|2021-04-26 17:07:00|\n",
      "|       12301|        GBRA|            GOBRA|2021-04-26 17:07:00|         7|2021-04-26 17:10:00|\n",
      "|       12301|         JOX|       JANAI ROAD|2021-04-26 17:10:00|         8|2021-04-26 17:11:00|\n",
      "|       12301|        BPAE|         BEGUMPUR|2021-04-26 17:11:00|         9|2021-04-26 17:14:00|\n",
      "|       12301|        BRPA|        BARUIPARA|2021-04-26 17:14:00|        10|2021-04-26 17:16:00|\n",
      "|       12301|         MBE|MIRZAPUR BANKIPUR|2021-04-26 17:16:00|        11|2021-04-26 17:17:00|\n",
      "|       12301|        BLAE|      BALARAMBATI|2021-04-26 17:17:00|        12|2021-04-26 17:19:00|\n",
      "|       12301|         KQU|       KAMARKUNDU|2021-04-26 17:19:00|        13|2021-04-26 17:21:00|\n",
      "|       12301|        MDSE|   MADHU SUDANPUR|2021-04-26 17:21:00|        14|2021-04-26 17:24:00|\n",
      "|       12301|        CDAE|       CHANDANPUR|2021-04-26 17:24:00|        15|2021-04-26 17:27:00|\n",
      "|       12301|         PBZ|        PORABAZAR|2021-04-26 17:27:00|        16|2021-04-26 17:28:00|\n",
      "|       12301|        BMAE|          BELMURI|2021-04-26 17:28:00|        17|2021-04-26 17:30:00|\n",
      "|       12301|        DNHL|      DHANIAKHALI|2021-04-26 17:30:00|        18|2021-04-26 17:32:00|\n",
      "|       12301|        SHBC|      SIBAICHANDI|2021-04-26 17:32:00|        19|2021-04-26 17:35:00|\n",
      "|       12301|         HIH|         HAJIGARH|2021-04-26 17:35:00|        20|2021-04-26 17:36:00|\n",
      "+------------+------------+-----------------+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding row numbers\n",
    "# Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , station_name, departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number, \n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM    schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e15671df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------------------------+-------------------+\n",
      "|train_number|station_code|unix_timestamp(departure, Yyyy-mm-dd)|   upcoming_arrival|\n",
      "+------------+------------+-------------------------------------+-------------------+\n",
      "|       12301|         HWH|                           1619436300|2021-04-26 16:58:00|\n",
      "|       12301|         LLH|                           1619436480|2021-04-26 17:00:00|\n",
      "|       12301|         BEQ|                           1619436600|2021-04-26 17:01:00|\n",
      "|       12301|         BLY|                           1619436660|2021-04-26 17:03:00|\n",
      "+------------+------------+-------------------------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding row numbers\n",
    "# Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , (UNIX_TIMESTAMP(departure, 'Yyyy-mm-dd')), \n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM    schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0786a1",
   "metadata": {},
   "source": [
    "### Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "36b6a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+\n",
      "|train_number|station_code|departure|row_number|\n",
      "+------------+------------+---------+----------+\n",
      "|       12301|         HWH| 16:55:00|         1|\n",
      "|       12301|         LLH| 16:58:00|         2|\n",
      "|       12301|         BEQ| 17:00:00|         3|\n",
      "|       12301|         BLY| 17:01:00|         4|\n",
      "|       12301|         BZL| 17:03:00|         5|\n",
      "+------------+------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVER Clause: Adding row numbers\n",
    "df.createOrReplaceTempView(\"schedules\")\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number\n",
    "        FROM schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "99513a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+----------------+\n",
      "|train_number|station_code|departure|row_number|upcoming_arrival|\n",
      "+------------+------------+---------+----------+----------------+\n",
      "|       12301|         HWH| 16:55:00|         1|        16:58:00|\n",
      "|       12301|         LLH| 16:58:00|         2|        17:00:00|\n",
      "|       12301|         BEQ| 17:00:00|         3|        17:01:00|\n",
      "|       12301|         BLY| 17:01:00|         4|        17:03:00|\n",
      "|       12301|         BZL| 17:03:00|         5|        17:05:00|\n",
      "+------------+------------+---------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEAD Clause: Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number,\n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e75dab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`upcoming_arrival`' given input columns: [schedules.arrival, schedules.day, schedules.departure, schedules.id, schedules.station_code, schedules.station_name, schedules.train_name, schedules.train_number]; line 2 pos 103;\n'Project [train_number#1285, station_code#1282, (unix_timestamp(departure#1339, Yyyy-mm-dd Hh:mm:ss, Some(Asia/Kolkata), false) - 'UNIX_TIMESTAMP('upcoming_arrival, Yyyy-mm-dd Hh:mm:ss)) AS diff#1619]\n+- Filter (cast(train_number#1285 as int) = 12301)\n   +- SubqueryAlias schedules\n      +- Project [to_timestamp('arrival, None) AS arrival#1348, day#1279L, departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n         +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n            +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1294, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n               +- Relation[arrival#1278,day#1279L,departure#1280,id#1281L,station_code#1282,station_name#1283,train_name#1284,train_number#1285] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-d2337a2f274d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mschedules_with_upcoming_arrival\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`upcoming_arrival`' given input columns: [schedules.arrival, schedules.day, schedules.departure, schedules.id, schedules.station_code, schedules.station_name, schedules.train_name, schedules.train_number]; line 2 pos 103;\n'Project [train_number#1285, station_code#1282, (unix_timestamp(departure#1339, Yyyy-mm-dd Hh:mm:ss, Some(Asia/Kolkata), false) - 'UNIX_TIMESTAMP('upcoming_arrival, Yyyy-mm-dd Hh:mm:ss)) AS diff#1619]\n+- Filter (cast(train_number#1285 as int) = 12301)\n   +- SubqueryAlias schedules\n      +- Project [to_timestamp('arrival, None) AS arrival#1348, day#1279L, departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n         +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n            +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1294, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n               +- Relation[arrival#1278,day#1279L,departure#1280,id#1281L,station_code#1282,station_name#1283,train_name#1284,train_number#1285] json\n"
     ]
    }
   ],
   "source": [
    "# Adding row numbers\n",
    "# Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , (UNIX_TIMESTAMP(departure, 'Yyyy-mm-dd Hh:mm:ss')- (UNIX_TIMESTAMP(upcoming_arrival, 'Yyyy-mm-dd Hh:mm:ss'))) AS diff \n",
    "        FROM    schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "schedules_with_upcoming_arrival= spark.sql(query).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfabbb52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`upcoming_arrival`' given input columns: [schedules_with_upcoming_arrival.arrival, schedules_with_upcoming_arrival.day, schedules_with_upcoming_arrival.departure, schedules_with_upcoming_arrival.id, schedules_with_upcoming_arrival.station_code, schedules_with_upcoming_arrival.station_name, schedules_with_upcoming_arrival.train_name, schedules_with_upcoming_arrival.train_number]; line 2 pos 113;\n'Project [train_number#1285, station_code#1282, departure#1339, unresolvedalias((unix_timestamp(departure#1339, Yyyy-mm-dd HH:mm:ss, Some(Asia/Kolkata), false) - 'UNIX_TIMESTAMP('upcoming_arrival, Yyyy-mm-dd HH:mm:ss)), None)]\n+- Filter (cast(train_number#1285 as int) = 12301)\n   +- SubqueryAlias schedules_with_upcoming_arrival\n      +- Project [to_timestamp('arrival, None) AS arrival#1348, day#1279L, departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n         +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n            +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1294, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n               +- Relation[arrival#1278,day#1279L,departure#1280,id#1281L,station_code#1282,station_name#1283,train_name#1284,train_number#1285] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-5d6d1f4896fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`upcoming_arrival`' given input columns: [schedules_with_upcoming_arrival.arrival, schedules_with_upcoming_arrival.day, schedules_with_upcoming_arrival.departure, schedules_with_upcoming_arrival.id, schedules_with_upcoming_arrival.station_code, schedules_with_upcoming_arrival.station_name, schedules_with_upcoming_arrival.train_name, schedules_with_upcoming_arrival.train_number]; line 2 pos 113;\n'Project [train_number#1285, station_code#1282, departure#1339, unresolvedalias((unix_timestamp(departure#1339, Yyyy-mm-dd HH:mm:ss, Some(Asia/Kolkata), false) - 'UNIX_TIMESTAMP('upcoming_arrival, Yyyy-mm-dd HH:mm:ss)), None)]\n+- Filter (cast(train_number#1285 as int) = 12301)\n   +- SubqueryAlias schedules_with_upcoming_arrival\n      +- Project [to_timestamp('arrival, None) AS arrival#1348, day#1279L, departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n         +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1339, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n            +- Project [arrival#1278, day#1279L, to_timestamp('departure, None) AS departure#1294, id#1281L, station_code#1282, station_name#1283, train_name#1284, train_number#1285]\n               +- Relation[arrival#1278,day#1279L,departure#1280,id#1281L,station_code#1282,station_name#1283,train_name#1284,train_number#1285] json\n"
     ]
    }
   ],
   "source": [
    "# Create temporary table called schedules\n",
    "df.createOrReplaceTempView(\"schedules_with_upcoming_arrival\")\n",
    "\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , departure, (UNIX_TIMESTAMP(departure, 'Yyyy-mm-dd HH:mm:ss')- UNIX_TIMESTAMP(upcoming_arrival, 'Yyyy-mm-dd HH:mm:ss'))\n",
    "        FROM schedules_with_upcoming_arrival\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bca5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
