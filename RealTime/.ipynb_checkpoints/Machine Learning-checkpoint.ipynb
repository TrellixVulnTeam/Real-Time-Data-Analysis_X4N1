{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning & More. Session I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import time\n",
    "import operator\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark-basic\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waking up & PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhatIsThisDoing0(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return x * WhatIsThisDoing0(x-1)\n",
    "\n",
    "3 * 2 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhatIsThisDoing1(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return WhatIsThisDoing1(x-1) + WhatIsThisDoing1(x-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhatIsThisDoing2(x):\n",
    "    a, b = 0, 1\n",
    "    for i in range(x):\n",
    "        a, b = b, a + b\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicty = {0:0, 1:1}\n",
    "\n",
    "def WhatIsThisDoing3(x):\n",
    "    if not x in dicty:\n",
    "        dicty[x] = WhatIsThisDoing3(x-1) + WhatIsThisDoing3(x-2)\n",
    "    return dicty[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhatIsThisDoing5(df, A, B):\n",
    "    for a in A: \n",
    "        df = df.withColumn(a, df[a].cast(B))\n",
    "    return df \n",
    "\n",
    "WhatIsThisDoing5(mydata, [\"col1\",\"col2\", \"col3\"], \"asFloat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we connect pyspark to a SQL Database (not-working examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: SQL vs NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "appName = \"PySpark SQL Server Example - via JDBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"sqljdbc_7.2/enu/mssql-jdbc-7.2.1.jre8.jar\") # LOCAL Driver\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "database = \"test\"\n",
    "table = \"dbo.Employees\"\n",
    "user = \"zeppelin\"\n",
    "password  = \"zeppelin\"\n",
    "\n",
    "jdbcDF = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database}\") \\ #LOCAL Database\n",
    "    .option(\"dbtable\", \"Employees\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windows Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymssql\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import _mssql\n",
    "import pandas as pd\n",
    "\n",
    "appName = \"PySpark SQL Server Example - via pymssql\"\n",
    "master = \"local\"\n",
    "\n",
    "conf = SparkConf().setAppName(appName).setMaster(master) \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "database = \"test\"\n",
    "table = \"dbo.Employees\"\n",
    "user = \"zeppelin\"\n",
    "password  = \"zeppelin\"\n",
    "\n",
    "conn = _mssql.connect(server='localhost:1433', user=user, password=password,database=database)\n",
    "query = f\"SELECT EmployeeID, EmployeeName, Position FROM {table}\"\n",
    "conn.execute_query(query)\n",
    "rs = [ row for row in conn ]\n",
    "pdf = pd.DataFrame(rs)\n",
    "sparkDF = spark.createDataFrame(pdf)\n",
    "sparkDF.show()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ODBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyodbc\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "appName = \"PySpark SQL Server Example - via ODBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "database = \"test\"\n",
    "table = \"dbo.Employees\"\n",
    "user = \"zeppelin\"\n",
    "password  = \"zeppelin\"\n",
    "\n",
    "conn = pyodbc.connect(f'DRIVER={{ODBC Driver 13 for SQL Server}};SERVER=localhost,1433;DATABASE={database};UID={user};PWD={password}')\n",
    "query = f\"SELECT EmployeeID, EmployeeName, Position FROM {table}\"\n",
    "pdf = pd.read_sql(query, conn)\n",
    "sparkDF =  spark.createDataFrame(pdf)\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prior questions...\n",
    "\n",
    "- What is Machine Learning?\n",
    "- What's the difference between ML and AI?\n",
    "- What's the difference between ML and BI?\n",
    "- Which areas of ML do you know?\n",
    "- How would you define a pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Machine Learning Pipeline. Full Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cricket Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: integer (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: integer (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Isball: boolean (nullable = true)\n",
      " |-- Isboundary: integer (nullable = true)\n",
      " |-- Iswicket: integer (nullable = true)\n",
      " |-- Over: double (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data = sqlContext.read.csv('data\\ind-ban-comment.csv',header=True,inferSchema=True)\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining manually the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: integer (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: integer (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Isball: boolean (nullable = true)\n",
      " |-- Isboundary: integer (nullable = true)\n",
      " |-- Iswicket: integer (nullable = true)\n",
      " |-- Over: double (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as tp\n",
    "\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True), \n",
    "    tp.StructField(name= 'Isboundary',   dataType= tp.IntegerType(),   nullable= True), # Should be Boolean, but...\n",
    "    tp.StructField(name= 'Iswicket',     dataType= tp.IntegerType(),   nullable= True), # Should be Boolean, but...\n",
    "    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),\n",
    "    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    \n",
    "])\n",
    "\n",
    "my_data = sqlContext.read.csv('data\\ind-ban-comment.csv',header= True,schema= my_schema)\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data.drop(*['Batsman', 'Bowler', 'Id']) # Dropping unnecessary columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Find a way to check how many nulls we find in each column. Plus: Are you able to do it in one line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-95d34f0d8e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Your Code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "### Your Code\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding our text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------------+------------+\n",
      "|     Batsman_Name|Batsman_Index|       Bowler_Name|Bowler_Index|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "SI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')\n",
    "SI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')\n",
    "\n",
    "# transform the data\n",
    "my_data = SI_batsman.fit(my_data).transform(my_data)\n",
    "my_data = SI_bowler.fit(my_data).transform(my_data)\n",
    "\n",
    "# view the transformed data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that OneHotEncoder is an old version of OneHotEncoderEstimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|     Batsman_Name|Batsman_Index|    Batsman_OHE|       Bowler_Name|Bowler_Index|    Bowler_OHE|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create object and specify input and output column\n",
    "OHE = OneHotEncoder(inputCols=['Batsman_Index', 'Bowler_Index'],outputCols=['Batsman_OHE', 'Bowler_OHE'])\n",
    "\n",
    "# transform the data\n",
    "my_data = OHE.fit(my_data).transform(my_data)\n",
    "\n",
    "# view and transform the data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)\n",
    "\n",
    "# Linear regression <- Betas depend on scale\n",
    "# RF <- Importance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this output showing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              vector|\n",
      "+--------------------+\n",
      "|(36,[1,2,4,24,25]...|\n",
      "|(36,[1,2,3,4,22,2...|\n",
      "|(36,[2,3,4,24,25]...|\n",
      "|(36,[2,3,4,22,25]...|\n",
      "|(36,[1,2,4,13,25]...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "|(36,[0,2,3,4,5,13...|\n",
      "|(36,[2,4,5,13,33]...|\n",
      "|(36,[2,4,5,13,33]...|\n",
      "|(36,[0,2,3,4,5,13...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "|(36,[2,4,22,25],[...|\n",
      "|(36,[2,3,4,13,25]...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,3,4,22,25]...|\n",
      "|(36,[1,2,4,19,25]...|\n",
      "|(36,[2,3,4,13,25]...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=['Isboundary', # Needs to be transformed to Integer\n",
    "                                       'Iswicket', # Needs to be transformed to Integer\n",
    "                                       'Over',\n",
    "                                       'Runs',\n",
    "                                       'Batsman_Index',\n",
    "                                       'Bowler_Index',\n",
    "                                       'Batsman_OHE',\n",
    "                                       'Bowler_OHE'],\n",
    "                           outputCol='vector')\n",
    "\n",
    "# fill the null values\n",
    "my_data = my_data.fillna(0)\n",
    "\n",
    "final_data = assembler.transform(my_data)\n",
    "final_data.select('vector').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But... Where are the Pipelines?! \n",
    "\n",
    "Let's see our **first toy example** ! Comment the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "sample_df = spark.createDataFrame([\n",
    "    (1, 'L101', 'R'),\n",
    "    (2, 'L201', 'C'),\n",
    "    (3, 'D111', 'R'),\n",
    "    (4, 'F210', 'R'),\n",
    "    (5, 'D110', 'C')\n",
    "], ['id', 'category_1', 'category_2'])\n",
    "\n",
    "sample_df.show()\n",
    "\n",
    "stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')\n",
    "stage_2 = StringIndexer(inputCol= 'category_2', outputCol= 'category_2_index') # 1,2,1,1,2\n",
    "stage_3 = OneHotEncoder(inputCols=['category_2_index'], outputCols=['category_2_OHE']) # 1,0,1,1,0 / # 0,1,0,0,1\n",
    "\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n",
    "\n",
    "pipeline_model = pipeline.fit(sample_df)\n",
    "sample_df_updated = pipeline_model.transform(sample_df)\n",
    "\n",
    "sample_df_updated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a schema of the pipeline that transforms the original data into the final one\n",
    "\n",
    "Rawdata.py -> preprocessdata.py -> ([listfeatures], int (number missings)) featuring.py  ([listfeatures2], int (number missings), int listnewfeat) -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with a-bit-more-complex **second toy example**! Machine Learning appears!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "sample_data_train = sqlContext.createDataFrame([\n",
    "    (2.0, 'A', 'S10', 40, 1.0),\n",
    "    (1.0, 'X', 'E10', 25, 1.0),\n",
    "    (4.0, 'X', 'S20', 10, 0.0),\n",
    "    (3.0, 'Z', 'S10', 20, 0.0),\n",
    "    (4.0, 'A', 'E10', 30, 1.0),\n",
    "    (2.0, 'Z', 'S10', 40, 0.0),\n",
    "    (5.0, 'X', 'D10', 10, 1.0),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])\n",
    "\n",
    "sample_data_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if wantencoding == True:\n",
    "    stage_1 = StringIndexer(inputCol= 'feature_2', outputCol= 'feature_2_index')\n",
    "else:\n",
    "    stage_1 = StringIndexer(inputCol= 'feature_2_mod', outputCol= 'feature_2_mod_index')\n",
    "\n",
    "\n",
    "\n",
    "stage_2 = StringIndexer(inputCol= 'feature_3', outputCol= 'feature_3_index')\n",
    "\n",
    "stage_3 = OneHotEncoder(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], \n",
    "                                 outputCols= ['feature_2_encoded', 'feature_3_encoded'])\n",
    "\n",
    "stage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'],\n",
    "                          outputCol='features')\n",
    "\n",
    "stage_5 = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5])\n",
    "\n",
    "model = regression_pipeline.fit(sample_data_train)\n",
    "\n",
    "sample_data_train = model.transform(sample_data_train)\n",
    "\n",
    "sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a schema of the pipeline that transforms the original data into the final one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, FINALLY, we can use the pipeline for a real test example! Check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data_test = sqlContext.createDataFrame([\n",
    "    (3.0, 'Z', 'S10', 40),\n",
    "    (1.0, 'X', 'E10', 20),\n",
    "    (4.0, 'A', 'S20', 10),\n",
    "    (3.0, 'A', 'S10', 20),\n",
    "    (4.0, 'X', 'D10', 30),\n",
    "    (1.0, 'Z', 'E10', 20),\n",
    "    (4.0, 'A', 'S10', 30),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n",
    "\n",
    "sample_data_test = model.transform(sample_data_test) #this model was trained with the train data\n",
    "\n",
    "sample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. (More BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what would have happened if we didn't infer the Schema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Comment the following code! Why do we need it? What are A and B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: float (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: float (nullable = true)\n",
      " |-- capital-loss: float (nullable = true)\n",
      " |-- hours-per-week: float (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_wrong = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= False)\n",
    "\n",
    "def WhatIsThisDoing(df, A, B):\n",
    "    for a in A: \n",
    "        df = df.withColumn(a, df[a].cast(B))\n",
    "    return df \n",
    "\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital-gain', 'educational-num', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "df_wrong = WhatIsThisDoing(df_wrong, CONTI_FEATURES, FloatType())\n",
    "\n",
    "df_wrong.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(education='11th', educational-num=7.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='Assoc-acdm', educational-num=12.0),\n",
       " Row(education='Some-college', educational-num=10.0),\n",
       " Row(education='Some-college', educational-num=10.0),\n",
       " Row(education='10th', educational-num=6.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='Prof-school', educational-num=15.0),\n",
       " Row(education='Some-college', educational-num=10.0),\n",
       " Row(education='7th-8th', educational-num=4.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='Bachelors', educational-num=13.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='Masters', educational-num=14.0),\n",
       " Row(education='Some-college', educational-num=10.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='HS-grad', educational-num=9.0),\n",
       " Row(education='Doctorate', educational-num=16.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wrong.select([\"education\",\"educational-num\"]).take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brush up: Create a summary of the number of people by education level, ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are some native countries with few observations. Discuss how valuable are these rows! Should we delete them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data. \n",
    "Exercise. Feature Engineering, data cleaning and summarizing the data\n",
    "\n",
    "Do your best :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. (more ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the Cricket Example to the Pipeline structure! Add some data cleaning, relabeling, feature engineering, scaling...And merge it up! After that, explore some machine learning algorithm to predict one of the meaningful events of the dataframe (isball, Runs...), using a train/test split and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were following these amazing tutorials: \n",
    " - https://www.guru99.com/pyspark-tutorial.html  \n",
    " - https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
